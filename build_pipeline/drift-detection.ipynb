{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drift Detection  Notebook\n",
    "\n",
    "This notebook will exercise the drift detection MLOps pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "👇 Set the project name for your drift pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"<<project_name>>\"  # << Update this drift detection project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get back the project id and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "region_name = sess._region_name\n",
    "sm_client = sess.sagemaker_client\n",
    "project_id = sm_client.describe_project(ProjectName=project_name)[\"ProjectId\"]\n",
    "\n",
    "print(f\"Project: {project_name} ({project_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "Let's copy some trip data and taxi zone files to the input location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "# Download trip data and taxi zones to input folder\n",
    "download_uri = \"s3://nyc-tlc/trip data/green_tripdata_2018-02.csv\"\n",
    "S3Downloader().download(download_uri, \"input/data\")\n",
    "download_uri = \"s3://nyc-tlc/misc/taxi_zones.zip\"\n",
    "S3Downloader().download(download_uri, \"input/zones\")\n",
    "\n",
    "# Upload input to the target location\n",
    "artifact_bucket = f\"sagemaker-project-{project_id}-{region_name}\"\n",
    "input_data_uri = f\"s3://{artifact_bucket}/{project_id}/input\"\n",
    "S3Uploader().upload(\"input\", input_data_uri)\n",
    "\n",
    "print(\"Listing input files:\")\n",
    "for s3_uri in S3Downloader.list(input_data_uri):\n",
    "    print(s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Start the pipeline now that we have uploaded some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"{project_name}-pipeline\"\n",
    "pipeline = Pipeline(pipeline_name)\n",
    "\n",
    "# Start pipeline\n",
    "execution = pipeline.start()\n",
    "execution_name = execution.arn.split(\"/\")[-1]\n",
    "\n",
    "print(f\"Waiting for execution: {execution_name} for pipeline {pipeline_name}...\")\n",
    "execution.wait()\n",
    "execution_status = execution.describe()[\"PipelineExecutionStatus\"]\n",
    "print(f\"Status: {execution_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the execution steps.  Note that we have baseline and training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in execution.list_steps():\n",
    "    print(\"Step: {}, Status: {}\".format(step[\"StepName\"], step[\"StepStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Get the estimator for the training job in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "def get_execution_step(step_name):\n",
    "    return [\n",
    "        step[\"Metadata\"]\n",
    "        for step in execution.list_steps()\n",
    "        if step[\"StepName\"] == step_name\n",
    "    ]\n",
    "\n",
    "\n",
    "training_job_arn = get_execution_step(\"TrainModel\")[0][\"TrainingJob\"][\"Arn\"]\n",
    "training_job_name = training_job_arn.split(\"/\")[-1]\n",
    "estimator = Estimator.attach(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Debugger XGBoost training report\n",
    "\n",
    "SageMaker Debugger generates a [XGBoost Training Report](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-training-xgboost-report.html) by a processing jobs that run concurrent to the training job. Let's wait for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name of the xgboost training report\n",
    "xgb_report_job_name = [\n",
    "    rule[\"RuleEvaluationJobArn\"].split(\"/\")[-1]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"CreateXgboostReport\" in rule[\"RuleConfigurationName\"]\n",
    "][0]\n",
    "\n",
    "print(\"Waiting for XGBoost training report to complete...\")\n",
    "sm_client.get_waiter(\"processing_job_completed_or_stopped\").wait(\n",
    "    ProcessingJobName=xgb_report_job_name\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ℹ️ The code below will download the output from the Debugger report in the `report` folder.  Click the link to open the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "# Get the s3 output\n",
    "report_uri = sm_client.describe_processing_job(ProcessingJobName=xgb_report_job_name)[\n",
    "    \"ProcessingOutputConfig\"\n",
    "][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "\n",
    "# Download the notebook from the report\n",
    "S3Downloader().download(f\"{report_uri}/xgboost_report.html\", \"report\")\n",
    "FileLink(\"report/xgboost_report.html\", result_html_prefix=\"Open Report: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approve Model\n",
    "\n",
    "🛑 Once we are happy with this training job, we can [Update the Approval Status](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-approve.html) of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn = get_execution_step(\"RegisterModel\")[0][\"RegisterModel\"][\"Arn\"]\n",
    "model_package_version = model_package_arn.split(\"/\")[-1]\n",
    "print(f\"Model version: {model_package_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update the status to approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_update_input_dict = {\n",
    "    \"ModelPackageArn\": model_package_arn,\n",
    "    \"ModelApprovalStatus\": \"Approved\",\n",
    "}\n",
    "model_package_update_response = sm_client.update_model_package(\n",
    "    **model_package_update_input_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "Now that our model is approve, the deployment pipeline will kick off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Define the predictor for staging\n",
    "def get_predictor(stage_name):\n",
    "    endpoint_name = f\"sagemaker-{project_name}-{stage_name}\"\n",
    "    return Predictor(\n",
    "        endpoint_name, serializer=CSVSerializer(), deserializer=JSONDeserializer()\n",
    "    )\n",
    "\n",
    "\n",
    "predictor = get_predictor(\"staging\")\n",
    "\n",
    "print(f\"Waiting for staging endpoint: {predictor.endpoint_name} to be deployed...\")\n",
    "sm_client.get_waiter(\"endpoint_in_service\").wait(EndpointName=predictor.endpoint_name)\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Staging\n",
    "\n",
    "Let's send some traffic to the staging endpoint with the following payload:\n",
    "\n",
    "| passenger_count\t| pickup_latitude\t| pickup_longitude\t| dropoff_latitude\t| dropoff_longitude\t| geo_distance\t| hour\t| weekday\t| month |\n",
    "| -| - | - | - | - | - | - | - | - |\n",
    "| 1\t| -73.986114\t| 40.685634\t| -73.936794\t| 40.715370\t| 5.318025\t| 7\t| 0\t| 2 |\n",
    "\n",
    "We expect approximately a $20 fare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = \"1,-73.986114,40.685634,-73.936794,40.715370,5.318025,7,0,2\"\n",
    "predictor.predict(data=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approve Staging\n",
    "\n",
    "🛑 Head over to the AWS Code Pipeline and approve the staging deployment to kick off the production deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Production\n",
    "\n",
    "Let's load our production endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import WaiterError\n",
    "\n",
    "try:\n",
    "    predictor = get_predictor(\"prod\")\n",
    "    print(f\"Waiting for prod endpoint: {predictor.endpoint_name} to be deployed...\")\n",
    "    sm_client.get_waiter(\"endpoint_in_service\").wait(\n",
    "        EndpointName=predictor.endpoint_name\n",
    "    )\n",
    "    print(\"Ready\")\n",
    "except WaiterError as err:\n",
    "    error_message = err.last_response[\"Error\"][\"Message\"]\n",
    "    if error_message.startswith(\"Could not find endpoint\"):\n",
    "        err = Exception(\"Approve Staging deployment to start Production deployment\")\n",
    "    raise err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And confirm that data capture is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)[\n",
    "    \"DataCaptureConfig\"\n",
    "]\n",
    "print(f\"Data capture is: {data_capture['CaptureStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Data Capture\n",
    "\n",
    "Let's send some traffic to the producition endpoint, which our [Data Quality Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html) should detect as drifting from the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for n in range(100):\n",
    "    predictor.predict(data=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we have received some outputs to our data capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_uri = data_capture[\"DestinationS3Uri\"]\n",
    "data_capture_files = S3Downloader.list(data_capture_uri)\n",
    "\n",
    "print(\"Found {} files\".format(len(data_capture_files)))\n",
    "\n",
    "if data_capture[\"EnableCapture\"] and len(data_capture_files) > 0:\n",
    "    # Get the first line of the most recent file\n",
    "    event = json.loads(S3Downloader.read_file(data_capture_files[-1]).split(\"\\n\")[0])\n",
    "    print(\"\\nLast file:\\n{}\".format(json.dumps(event, indent=2)))\n",
    "elif len(data_capture_files) == 0:\n",
    "    print(\"No files yet, please rerun this cell in a few seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download the validation dataset from the latest processing job, and tweak some of the columns to change the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import random\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "\n",
    "def get_latest_processed_data():\n",
    "    execution_arn = sm_client.list_pipeline_executions(\n",
    "        PipelineName=pipeline_name, SortBy=\"CreationTime\"\n",
    "    )[\"PipelineExecutionSummaries\"][0][\"PipelineExecutionArn\"]\n",
    "    steps = sm_client.list_pipeline_execution_steps(\n",
    "        PipelineExecutionArn=execution_arn, SortOrder=\"Ascending\"\n",
    "    )[\"PipelineExecutionSteps\"]\n",
    "    preprocess_arn = next(\n",
    "        item[\"Metadata\"][\"ProcessingJob\"][\"Arn\"]\n",
    "        for item in steps\n",
    "        if item[\"StepName\"] == \"PreprocessData\"\n",
    "    )\n",
    "    job_outputs = sm_client.describe_processing_job(\n",
    "        ProcessingJobName=preprocess_arn.split(\"/\")[1]\n",
    "    )[\"ProcessingOutputConfig\"][\"Outputs\"]\n",
    "    validation_uri = next(\n",
    "        item[\"S3Output\"][\"S3Uri\"]\n",
    "        for item in job_outputs\n",
    "        if item[\"OutputName\"] == \"validation\"\n",
    "    )\n",
    "    return validation_uri\n",
    "\n",
    "\n",
    "dataset_location = get_latest_processed_data()\n",
    "S3Downloader().download(dataset_location, \"prerocessed\")\n",
    "df = pd.read_csv(\"prerocessed/validation.csv\", header=None)\n",
    "\n",
    "# Changing the distribution of data to artificially cause an alarm\n",
    "df[1] = random.choices([1, 2, 3, 4, 5, 6], weights=[2, 1, 2, 5, 2, 1], k=df.shape[0])\n",
    "df[6] = df[1].apply(lambda x: 70 * random.betavariate(2.5, 2))\n",
    "tweaked_rows = df.drop(0, axis=1).to_csv(header=False, index=False).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make a series of prediction requests with this data to cause an artificial model monitoring alarm to be triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "endpoint_requests = 10000\n",
    "for i in range(endpoint_requests):\n",
    "    predictor.predict(data=tweaked_rows[i % len(tweaked_rows)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor\n",
    "\n",
    "Let's check that we have a monitor configured and that its schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "model_monitor = predictor.list_monitors()[0]\n",
    "model_monitor_status = model_monitor.describe_schedule()[\"MonitoringScheduleStatus\"]\n",
    "print(f\"Model Monitoring: {model_monitor_status}\")\n",
    "\n",
    "now = datetime.now(tzlocal())\n",
    "next_hour = (now + timedelta(hours=1)).replace(minute=0)\n",
    "scheduled_diff = (next_hour - now).seconds // 60\n",
    "print(\"Next schedule in {} minutes\".format(scheduled_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the latest execution and output the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_executions = model_monitor.list_executions()\n",
    "if len(monitor_executions) == 0:\n",
    "    raise (Exception(\"Please wait, no monitor executions available yet\"))\n",
    "\n",
    "# Get the latest monitor status\n",
    "monitor_status = monitor_executions[0].describe()[\"ProcessingJobStatus\"]\n",
    "if monitor_status == \"Completed\":\n",
    "    monitor_message = monitor_executions[0].describe()[\"ExitMessage\"]\n",
    "    print(f\"Latest execution: {monitor_message}\")\n",
    "else:\n",
    "    print(f\"Latest execution: {monitor_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Model Monitor report\n",
    "\n",
    "🛑 Browse to the model monitoring results in SageMaker Studio to download and run a report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain\n",
    "\n",
    "The monitoring schedule will have trigger a detection alarm will have triggered a new pipeline to retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_pipeline_execution = sm_client.list_pipeline_executions(\n",
    "    PipelineName=pipeline_name,\n",
    ")[\"PipelineExecutionSummaries\"][0]\n",
    "latest_execution_status = latest_pipeline_execution[\"PipelineExecutionStatus\"]\n",
    "time_ago = datetime.now(tzlocal()) - latest_pipeline_execution[\"StartTime\"]\n",
    "\n",
    "print(\n",
    "    f\"Latest pipeline: {pipeline_name} execution: {latest_execution_status} started {time_ago.total_seconds()/60:0.2f} mins ago\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's list the steps of that execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_steps = sm_client.list_pipeline_execution_steps(\n",
    "    PipelineExecutionArn=latest_pipeline_execution[\"PipelineExecutionArn\"],\n",
    ")[\"PipelineExecutionSteps\"]\n",
    "for step in execution_steps:\n",
    "    print(\"Step: {}, Status: {}\".format(step[\"StepName\"], step[\"StepStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Great now you have completed all the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Execute the following cell to delete any registered models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.list_model_packages(ModelPackageGroupName=project_name)\n",
    "for model_package in response[\"ModelPackageSummaryList\"]:\n",
    "    print(\"Deleting Version {}\".format(model_package[\"ModelPackageArn\"].split(\"/\")[-1]))\n",
    "    sm_client.delete_model_package(ModelPackageName=model_package[\"ModelPackageArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to delete cloudformation stacks\n",
    "\n",
    "1. SageMaker prod endpoint\n",
    "2. SageMaker staging endpoint\n",
    "3. SageMaker Pipeline Workflow and Model Package Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cfn = boto3.client(\"cloudformation\")\n",
    "\n",
    "for stack_name in [\n",
    "    f\"sagemaker-{project_name}-deploy-prod\",\n",
    "    f\"sagemaker-{project_name}-deploy-staging\",\n",
    "    f\"sagemaker-{project_name}-pipeline\",\n",
    "]:\n",
    "    print(\"Deleting stack: {}\".format(stack_name))\n",
    "    cfn.delete_stack(StackName=stack_name)\n",
    "    cfn.get_waiter(\"stack_delete_complete\").wait(StackName=stack_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will clean up all objects in the artifact bucket and delete the SageMaker project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_artifact_bucket = s3_resource.Bucket(artifact_bucket)\n",
    "s3_artifact_bucket.object_versions.delete()\n",
    "print(\"Artifact bucket objects deleted\")\n",
    "\n",
    "sm_client.delete_project(ProjectName=project_name)\n",
    "print(\"SageMaker Project deleted\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
